renderer_kwargs:
  dist: 0.5
  elev: 0.0
  azim: 0.0
  img_size:
  - 1024
  - 1024
  tex_path: null
  texture_optimization: true
train_kwargs:
  hydra:
    run:
      dir: /home/nadav2/dev/repos/CLIP2Shape/models/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  models_dir: /home/nadav2/dev/repos/CLIP2Shape/models
  seed: 42
  tensorboard_logger:
    name: check_of_algo_flame_shape
    save_dir: /home/nadav2/dev/data/CLIP2Shape/runs
    log_graph: true
  train_size: 0.8
  callbacks:
    model_checkpoint:
      _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val_loss
      mode: min
      save_top_k: 1
      verbose: false
      dirpath: /home/nadav2/dev/repos/CLIP2Shape/models
      filename: epoch_{epoch:03d}
      auto_insert_metric_name: false
  dataloader:
    batch_size: 200
    num_workers: 1
    shuffle: true
    drop_last: false
  trainer:
    gpus: 1
    max_epochs: 50
    check_val_every_n_epoch: 5
    log_every_n_steps: 3
  dataset:
    data_dir: /home/nadav2/dev/data/CLIP2Shape/images/flame_textured
    optimize_feature: shape_params
    out_features: 10
    labels_to_get:
    - fat
    - long neck
    - big forhead
  model_conf:
    num_stats: 3
    hidden_size:
    - 800
    - 500
    out_features: 10
    num_hiddens: 1
model_type: flame
gender: neutral
optimize_feature: expression_params
data_path: /home/nadav2/dev/data/CLIP2Shape/images/flame_textured_exp
descriptors_clusters_json: /home/nadav2/dev/data/CLIP2Shape/outs/clustering_images/words_jsons/flame_expression.json
batch_size: 2500
output_path: /home/nadav2/dev/data/CLIP2Shape/outs/descriptors_ablations/flame_expression
descriptors_options:
- 2
- 5
- 10
- 15
- 20
